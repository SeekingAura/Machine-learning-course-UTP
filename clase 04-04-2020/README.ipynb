{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo del perceptron\n",
    "\n",
    "* Dos clases. Los datos de entrada se transforman como $\\phi (\\textbf{x})$\n",
    "* El modelo lineal tiene la forma\n",
    "\n",
    "$y(\\textbf{x})=f(\\textbf{w} \\phi (\\textbf{x}))$\n",
    "\n",
    "Donde f() es la función escaló unitario\n",
    "\n",
    "* En el perceptrón se asume t=+1, para $C_1$ y $t=-1$ para $C_2$\n",
    "\n",
    "\n",
    "$\\{x_, t_n\\}; t_{n} \\in \\{+1, -1\\}$\n",
    "\n",
    "$w^{\\top}\\varphi >0$ -> Si es clase 1\n",
    "$w^{\\top}\\varphi <0$ -> Si es clase 2\n",
    "\n",
    "\n",
    "Si esta bien clasificado $\\varphi$ se cumple que $w^{\\top}\\varphi_{n}t_{n}>0$\n",
    "\n",
    "## Algoritmo  Perceptron 2\n",
    "* La función a minimizar se conoce como el criterio del perceptrón\n",
    "\n",
    "$E_{p}(\\textbf{w})=-\\sum_{n\\in M}\\textbf{w}^{\\top}\\phi (\\textbf{x}_n)t_{n}$\n",
    "\n",
    "\n",
    "## Minimizar\n",
    "Para minimizar si se usa directamente derivadas hay un problema al igualar a cero entonces se deberia utilizar otro método de minimización como el **gradiente desendiente**\n",
    "\n",
    "### El gradiente desendiente\n",
    "Este toma la función en cualquier punto y de ahi verifica como cambia hacia un lado u otro, si se reduce ese otro.\n",
    "\n",
    "De acuerdo a la tasa de aprendizaje que se recalcula\n",
    "\n",
    "\n",
    "* Inicializar a $w_0$\n",
    "* Definir la tasa de aprendizaje $\\Delta j(w_{z}n)$\n",
    "* Hacer un ciclo mientras No haya convergido... $w_{z+1}=w_{t}-\\Delta j(w_{z}n)$\n",
    "    * Cual el valor de $||\\Delta j||<tn$\n",
    "   \n",
    "\n",
    "\n",
    "## Gradiente uso descendiente\n",
    "* Aplicando el algoritmo de gradiente descendiente estocástico a esta función, se tiene:\n",
    "\n",
    "$\\textbf{w}^{(\\tau +1)}=\\textbf{w}^{(\\tau)}-n\\Delta E_p(\\textbf{w}=...)$\n",
    "\n",
    "El gradiente estocastico funciona por lotes, donde se hacen derivadas por partes de todo el conjunto de datos hasta que logre convergencia\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo del perceptron 3\n",
    "De unos datos ya clasificados se toman los que estan mal clasificados se les aplica el algoritmo para recalcular su nueva posición y su linea de clasificación\n",
    "\n",
    "Mediante el uso de \"flechas\" se establece cual es el area de clasificación\n",
    "\n",
    "La corrección que se aplica es \"recalcular\" el valor de uno de los datos mal clasificados mediante el **gradiente desendiene estocastico**)\n",
    "\n",
    "### Ventajas\n",
    "* Este garantiza que haya convergencia que el sistema sea linealmente separable\n",
    "\n",
    "* A diferencia de la Maxima verosimilitud y los LAD estos dos son bastante costosos computacionalmente, en cambio este de **gradiente desendiente estocastico**\n",
    "\n",
    "### Desventajas\n",
    "* Si el problema no es linealmente separable (que no haya forma de separarlo con una linea en cada vez) no hay convergencia garantizada\n",
    "* No se sabe si la falta de convergencia es por que el problema no es linealmente separable\n",
    "* Existen infinitas soluciones es probable que alguna entre las posibles no se escoja la mejor que hay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos generativos probabilisticos\n",
    "\n",
    "$p(x,y)\\rightarrow p(x,p_k)$\n",
    "$y-> etiqueta$\n",
    "\n",
    "Los aprendizajes supervisados son aprendizajes donde siempre se sabe a donde se quiere llegar (se tiene un target).\n",
    "\n",
    "## Introducción\n",
    "* En los modelos generativos se modelan las densidades de clase condicional p(\\textbf{x}|C_{k}), y las funciones de probabildiad a priori, p(C_k)\n",
    "* Ambas probabildiades se usan para calcular el posterior p(C_k|\\textbf{x})\n",
    "\n",
    "$p(c_{k}|x) \\rightarrow \\sum _{k} p(C_{k}|x)=1$ -> probabilidad a posterior\n",
    "\n",
    "\n",
    "$p(x_{1}C_{k})=p(x|C_{n})p(C_{k})=p(C_k|x)$\n",
    "\n",
    "\n",
    "Mediante el posterior se pueden tomar decisiones a razón de su probabilidad mas alta\n",
    "\n",
    "\n",
    "## Modleo generativo 1\n",
    "\n",
    "Para el caso biclase (binario)\n",
    "\n",
    "$k=2 p(C_{1}|x)=\\frac{p(x|C_1)p(C_{1})}{p(x)}$\n",
    "\n",
    "Se debe obtener la probabilidad marginal del sistema, para ello se suban las probabilidades para que sea de clasificado de una clase y de la otra clase (son solo 2 clases)\n",
    "\n",
    "**Probabilidad de la evidencia**\n",
    "$p(x)= \\sum _{k} p(C_{x},x)=p(C_{1}, x)+p(c_{2},x)=p(x|c_1)$\n",
    "\n",
    "\n",
    "La probabildiad a posterior de la clase 1 es\n",
    "\n",
    "$p(C_{1}|\\textbf{x})=\\frac{1}{1+exp(-a)}=\\sigma(a)$\n",
    "\n",
    "$\\frac{1}{1+exp(-a)}$ es la función sigmoidal o sigmoide que se representa como $\\sigma$ tambien se le conoce como tangente hipervolica (versión suave del step)\n",
    "\n",
    "\n",
    "Sobre el posterior es que se utiliza para calcular las predicciones, por ende es a este que se debe calcular las probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modleo generativo 2\n",
    "\n",
    "### Función sigmoidal\n",
    "Es como una función de \"activación que tiene un acote de 0 y 1 donde a mayor mas cercano es a una clase, a menor mas cerca es a la segunda clase.\n",
    "\n",
    "Se debe averiguar cual es el $max{p(c_{1}|x)}$ determinando que tan probable es que sea de una clase o de otra, de hecho al determinar que tanto es de otra a su vez indica que tanto es de la otra\n",
    "\n",
    "$a=ln(p(x,c_{1}))$\n",
    "\n",
    "Si la probabilidad es el 50% es por que sus probabilidades conjuntas es 0 (valor **a**)\n",
    "\n",
    "Si el valor **a** del rango de vlaores es positivo es de una clase y si es negativo es de otra clase\n",
    "\n",
    "En caso de que el valor de **a** de cero se puede establecer un valor especifico de clasificación, de que sea uno o el otro o omitirlo. \n",
    "\n",
    "Tambien se puede establecer un umbral donde no clasifique si es de una clase u otra apartir de otro rango por ejemplo apartir de 30% si es superior a eso es de una clase y si es inferior es otra clase.\n",
    "\n",
    "Dependiendo al caso particular tambien se puede descartar los datos (no se tomen decisiones o no se asigne etiquetas, Borderline)\n",
    "\n",
    "Si el problema tiene mas de una clasificación la probabilidad de la evidencia es la suma de cada una de las probabilidades quedando. Función soft-max\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo generativo: entradas continuas\n",
    "* Clase condicional Gaussiana, matrix de covarianza igual para todas las clases\n",
    "$p(\\textbf{x}|C_{k})=\\frac{1}{(2\\pi )^{D/2}\\Sigma}$\n",
    "\n",
    "\n",
    "* Frontera en el modelo generativo j=? condicional=Gauss\n",
    "$o(x_{1}c_{1})=p(x_{1}c_{2})\\rightarrow  p(x|c_1)p(c_{1})=p(x|c_{2})p(c_{1})N(\\mu _{1},\\sigma _{1})\\pi = N(\\mu _{2}, \\sigma _{2})$\n",
    "\n",
    "\n",
    "....\n",
    "\n",
    "Se busca cual es la parte de \"linea\" que tiene esta función\n",
    "\n",
    "La frontera del modelo es una Cuadratica\n",
    "\n",
    "$X\\in \\mathbb{R}^{B}/ - \\frac{1}{2} x^{\\top}AX + b^{\\top}x=c$\n",
    "\n",
    "Las fronteras cuadraticas pueden ser un paraboloide o una hiperelipse. Esto permite resolver problemas que no son linealmente separables\n",
    "\n",
    "Para que deje de ser cuadratica debe tener su valor de A, para ello se debe de cumplir que $\\Sigma _{1}=\\Sigma _{2}=\\Sigma$ Las matrices de covarianza\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo generativo 3\n",
    "Si las clases no comparten la misma matriz de covarianza, las regiones de decisión son cuadraticas. Es decir forman figuras que son curvas o figuras delimitadas\n",
    "\n",
    "\n",
    "# Maxima verosimilitud\n",
    "* Los valores de $\\mu _{1}$, $\\mu _{2}$, $\\Sigma$,  y $p(C_{1})$ y $p(C_{2})$ se determinan usando la maxima verosimilitud\n",
    "* Son dos clases y un conjunto de datos $(x_{n}, t_{n})^{N}_{n=1}$\n",
    "* $t_{n}=1$ denota $C_{1}$ y $t_{n}$=0 Denota $C_{2}$\n",
    "\n",
    "La maxima verosimilitud es La probabildiad de xn como entrada y tn como salida dado TODOS los parametros\n",
    "\n",
    "## Maxima vero similitud 3\n",
    "Se puede calcular de dos formas dependiendo del valor de $t_{n}$\n",
    "\n",
    "Verosimilitud\n",
    "\n",
    "$p(t,X|\\pi,\\mu _{1},\\mu _{2},\\Sigma )= \\prod_{N}^{n=1}[ \\pi N(x _{n} | \\mu _{1}, \\Sigma) ]^{t _{n}} [ (1-\\pi) N(x_{n})| \\mu _{2}, \\Sigma)]$\n",
    "\n",
    "$p(c_{1}, x) == \\[ \\piN(x _{n} | \\mu _{1}, \\Sigma) \\]^{t _{n}}$\n",
    "\n",
    "\n",
    "Recordemso que la maxima verosimilitud debe ser minimizado y para ello es derivando, pero como ya se dijo antes derivar directamente es complciado eentonces se usa **estimación de parametros**\n",
    "\n",
    "$(1- \\pi) \\sum ^{N} _{n=1} t_{n}$\n",
    "\n",
    "### Prior de la clase 1\n",
    "$\\pi$ -> el Prior\n",
    "$1- \\pi$\n",
    "$\\pi = (\\frac{N_{1}}{N})$\n",
    "\n",
    "### Prior de la clase 2\n",
    "$1- \\pi$\n",
    "$\\pi = (\\frac{N_{2}}{N})$\n",
    "\n",
    "### Prior de la clase K\n",
    "$p(C_{k})=\\frac{N_{k}}{N}$\n",
    "\n",
    "\n",
    "La suma de todos los prior debe dar 1 por que son probabilidades\n",
    "\n",
    "### Centroide de la clase 1\n",
    "$\\frac{1}{N_{1}} \\sum ^{N}_{n=1} t_{n}x_{n}=\\frac{1}{n_{1}}$\n",
    "\n",
    "### Finalmente\n",
    "$\\Sigma = S = \\frac{N_{1}}{N}S_{1}+\\frac{N_{2}}{N}S_{2}$\n",
    "\n",
    "$S_{1}=\\frac{1}{N_{1}\\sum _{n \\in C_{1}}  }$\n",
    "\n",
    "\n",
    "### Calcular covarianza\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
