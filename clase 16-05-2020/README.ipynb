{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "## Kernel equivlaente del regreso Bayesiano lineal\n",
    "* Recordemos el modelo de regresión lineal Bayesiana para predecir la salida y de una entrada nueva (test) x a partir de los datos de entrenamiento $X \\in \\mathbb{R}^{NxD}$, $t\\in \\mathbb{R}^{N}$\n",
    "\n",
    "$m^{\\top }_{N}x=)$ -> media del posterior por los x data\n",
    "\n",
    "$\\Phi$ -> Matrix\n",
    "\n",
    "$y(x,m_{N}=m^{\\top }_{N}x=\\beta \\phi (x)^{\\top} S_{N}\\Phi \\top t = \\sum_{N}^{n=1} \\beta t_{n}\\phi (x_{n})^{\\top} S_{N}\\phi (x)$\n",
    "\n",
    "Donde $\\Phi \\in \\mathbb{R}^{N*P}$ es la **matriz de diseño** construida a partir del mapeo de cada muestra de entrenamiento, $\\phi (x) \\in \\mathbb{R}^{P}$ el mapeo de la muestra de test y $S_{N} \\in \\mathbb{R}^{P*P}$ la covarianza del posterior\n",
    "\n",
    "* De acuerdo a la descomposición en valores singulares, la covarianza del posterior se puede escribir como $S_{N}=U\\lambda U^{\\top} = (U \\lambda ^{\\frac{1}{2}})(\\lambda ^{\\frac{1}{2}}U^{\\top})=VV^{\\top}$\n",
    "\n",
    "La transpuesta es la transposición de los datos.\n",
    "\n",
    "cualquier matrix se peude descomponer de la forma\n",
    "\n",
    "$X = U \\lambda V$\n",
    "\n",
    "Si X es completamente definido $V=U^{\\top}$\n",
    "\n",
    "\n",
    "$y(x, m_{N}) = \\sum_{n=1}^{N}(\\beta ^{\\frac{1}{2}})(\\beta ^{\\frac{1}{2}})t_{n}\\phi (x_{n})^{\\top}vv^{\\top}\\phi (x) = \\sum_{n=1}^{N} t_{n}z(x_{b})^{\\top} z(x)$\n",
    "\n",
    "* La salida del regreso se puede interpretar de dos formas\n",
    "\n",
    "    * Como una combinación lineal de productos internos entre las muestras de entrenamiento transformadas y la muestra de test transformada. El producto interno de los vectores se utilziaba para detremrinar si son diferentes o iguales mediante la ortogonalidad\n",
    "    \n",
    "    $y(x, m_{n})= \\sum_{n=1}^{N} t_{n}z_{n}^{\\top}z : z(x)=\\beta ^{\\frac{1}{2}}V^{\\top} \\phi (x); V \\in \\mathbb{R}^{P*P}$\n",
    "    * Como una combinación lineal del resultado de una función, conocida como **kernel equivlaente** al producto itnerno en otro espacio. Esta se conoce como la representación dual:\n",
    "    \n",
    "    $y(x, m_{N})= \\sum_{n=1}^{N}t_{n}k(x_{n}, x); k(.,.): \\mathbb{R}^{D}*\\mathbb{R}^{D} \\rightarrow \\mathbb{R}$\n",
    "    \n",
    "    $k(x_{i}, x_{j})=\\beta \\phi (x_{i}^{\\top}) S_{N} \\phi (x_{j})$\n",
    "\n",
    "* La función kernel\n",
    "\n",
    "## Kernel equivalente para bases polinomiales de orden 2\n",
    "* Para el siguiente problema de clasificación binaria a partir de muestras en dos dimensiones $(x \\in \\mathbb{R}^{2})$ se proponen\n",
    "    \n",
    "El producto de las componentes determinan que tna parecidas son, por ejemplo si da 0 es que son ortognales  (no se parecen casi) si son iguales dará un producto muy alto (cuando son paralelas).\n",
    "\n",
    "\n",
    "* El producto interno en $\\phi$ se calcula como:\n",
    "\n",
    "$(\\phi (x_{i}), \\phi x_{j}))= \\phi_{i}^{\\top}\\phi _{j}$\n",
    "$(\\phi (x_{i}), \\phi x_{j}))=(x_{i}, x_{j})^{2}=k(x_{i}, x_{j})$ -> binomio cuadradado, un producto interno, la función k representa la operación del producto interno\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de kernel\n",
    "## Kernel equivlaente del regresor bayesiano lineal\n",
    "* Recordemos el modelo de regresión lineal \n",
    "\n",
    "* Primal en terminos e los parametros\n",
    "* La dual en termino de las muestras\n",
    "\n",
    "\n",
    "## Definición kernel\n",
    "* Una función kernel corresponde a un producto interno en un espacio de caracteristicas.\n",
    "* Muchos modelos parámetricos en un espacio de características\n",
    "* Muchos modelos paramétricos lineale stienen una representación dual equivalente en la cual las predicciones se puede escribir como una  **combinación lineal** de funciones kernel evaluadas en las muestras de entrenamiento.\n",
    "* Las máquinas basdas en kernels requieren almacenar muestras como parte de sus parámetros para poder hacer predicciones por lo que hacen parte de la familia de los memory-based methods\n",
    "* Las máquinas basdas en kernels re\n",
    "* Los métodos basados en memoria usualmente requeiren una métrica como medida de similitud entre dos vectores.\n",
    "\n",
    "\n",
    "Si el espacio P es menor que la cantidad de datos de entrenamiento /o sea la cantidad de clases es menor que la cantidad de datos) la representación dual seria demasiado costososa\n",
    "\n",
    "en un modelo bayesiano si no se calcula m, debe almacenarse las muestras (un metodo basado en memoria)\n",
    "\n",
    "todos lso modelos basados ne memoria requeire algun método de medida de similitud, de hecho el kernel es parte de eso\n",
    "\n",
    "\n",
    "* Si una meustra se representra dentro de una metr\n",
    "\n",
    "    * Linealidad\n",
    "    * Simetría\n",
    "    * Definición positiva: el producto interno contra el mismo siempre entregará positivo o cero\n",
    "    * Si reproduce una norma, $k(x,x)=||x||^{2}$ debe cumplir la desigualdad de cauchy Schawarz\n",
    "    \n",
    "Si se cumple los 4 postulados anteriores entonces, existe un kernel que emula su producto interno y su norma, este es un espacio de Hilbert (espacios vectoriales dotados de producto interno y norma) reproducido por un kernel, siendo los RKHS los espacios de hilbert reprocidos por algun punto. \n",
    "\n",
    "* Funcioens kernel tipicos\n",
    "    * Lineal: es el producto tal y cual producto interno $k(x_{i}, x_{j}=x_{i}^{\\top} x_{j})$\n",
    "    * Polinomial: Es el que usa el binomio cmo se calculo antes, de un producto interno al cuadrado\n",
    "    * Gaussiano (Radial Basis function - RBF) siendo una función de base radial $exp(-||x_{i}-x_{j}||^{2}/2\\sigma ^{2})$ donde $\\sigma \\in \\mathbb{R}^{+}$ es la escala del kernel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maquinas de vectores de soporte (SVM)\n",
    "\n",
    "## Clasificadores de máxima margen\n",
    "* Retomemos el modelo de clasificación binaria lineal $y(x)=w^{\\top} \\varphi (x)+b$ donde la decision es: $t* = sign(y(x))$ y w,b parametrizan el hiperplano de separación \n",
    "\n",
    "\n",
    "La distancia (valores x) no depende de la magnitud del vector (en clasificación cuando se grafica), solo dpenede de su orientación. \n",
    "\n",
    "Al modificar la magnitud el valor de y va cambiando de forma proporcional (a razón de las distancias), hay que recordar que y es la margen de selección.\n",
    "\n",
    "En la clasificación de estas maquinas cuando hay mas de una solución posible y es algo lineal se debe dar una margen entre la clasificación para que asi haya mejor presiciónd e la clasificación siendo la \"mejor\" clasificación utilizando esas otras funciones dando una \"margen\"\n",
    "\n",
    "* La margen es el valor $y(k_{n})$ de la muestra que está mas cerca a la frontera\n",
    "* Para saber que tan lejos (o cerca) de la frontera está una meustra, usamos la distancia de un punto a un (hiper)plano\n",
    "\n",
    "$d(x_{n}, w)=\\frac{t_{n}y_{n}}{||w||}=\\frac{t_{n}(w^{\\top}\\varphi (x_{n})+b)}{||w||}$\n",
    "\n",
    "\n",
    "El punto con la distancia mas pequeña es aquel que haga que la distancia en la base de datos sea minima, si los que estan mas cerca tienen una distnacia grande, los demas puntos han de tener una distancia aun mas grande.\n",
    "\n",
    "* Como se queire que la distancia sea lo más grande posible, una **protofunción** de costo para maximizar la margen es:\n",
    "\n",
    "$maxmind(x_{n}, w)$\n",
    "\n",
    "se moverá el vector w la muestra mas cercana sea 1, es decir\n",
    "\n",
    "$t_{n}y_{n}=t_{n}(w^{\\top} \\varphi (x_{n})+b)=1)$ -> Restricción, si esto se cumple para el minimo sobre n margen dura\n",
    "\n",
    "Lo que se queire es minimizar la norma del vector, o el cuadrado de la norma.\n",
    "\n",
    "El problema primal dependia de los parametros. El problema será\n",
    "\n",
    "$min \\frac{1}{2}||w||^{2}$\n",
    "\n",
    "sijeto a $s, t, t_{n}(w^{\\top} \\varphi (x_{n})+b) >= 1; n=1, ..., N$\n",
    "\n",
    "* Optimizar con restricciones con el Lagrangiano, las restricciones son a razón de las muestras\n",
    "a -> multiplicadores de la grange\n",
    "$L(w, b, a)=\\frac{1}{2}||w||^{2} - \\sum _{n=1} ^{N} a_{n}\\{ t_{n} y(x_{n}-1\\}$\n",
    "\n",
    "donde $a \\in \\mathbb{R}^{N}$ es el vector de los N multiplicadores de Lagrage (uno por cada restricción\n",
    "\n",
    "El problema dual que se encuentra para el L(w,b,a) es\n",
    "\n",
    "$L(a)=-\\frac{1}{2}a^{\\top}(T\\circ K)a + 1_{N}^{\\top}a$\n",
    "\n",
    "$s.T a>= 0$\n",
    "\n",
    "$t^{\\top}a=0$\n",
    "\n",
    "Esto es un problema al estilo SIMPLEX, \n",
    "\n",
    "$T\\circ K$ es una matrix gramma\n",
    "\n",
    "Este prolema corresponde a un problema de optimización cuadrática con restricciones de igualdad y desigualdad lineales.\n",
    "\n",
    "El problema dual en si es en un espacio en donde deben cumplirse esas restricciones, debe acercarse lo mas posible a ese optimo pero siempre cumpliendo las restricciones. en un plano cartesiano, si se tienen en cuenta las restricciones los valores de a deben de ser unicmaente ne el primer cuadrante donde **x** y **y** es positivo.\n",
    "\n",
    "* $T\\circ K$ es el producto de Haddamard (elemento a elemento)\n",
    "* $1_{n} \\in \\mathbb{R}^{N}$\n",
    "\n",
    "* Reemplazando la ecucación del vector se define que el hiperplano en el modelo:\n",
    "\n",
    "$y(x^{*})=w^{\\top}\\varphi (x^{*})+b$\n",
    "\n",
    "$y(x^{*})=\\sum _{n=1 ^{N}} a_{n} t_{n}k(x_{n}, x^{*})+b$ -> Las predicciones se hacen de este modo\n",
    "\n",
    "* Segun las condiciones de optimalidad KKT\n",
    "\n",
    "\n",
    "* Lo cual\n",
    "    * Si $a_{n}=0$, entonces $t_{n}y(x_{n})>1$ es decir, $x_{n}$ esta fuera de la margen y NO suma para las predicciones\n",
    "    * Si $a_{n}>0$, entonces $t_{n}y(x_{n})=1$, es decir, $x_{n}$ está en la margen y SI suma par alas predicciones, $x_{n}$ se vuelve \n",
    "    \n",
    "    \n",
    "en las predicciones se forman varios vectores de soporte, se peudne elegir un predeterminado numero de vectores, pero para clasificar bien sedeben escojer todos\n",
    "    \n",
    "Cuando se va hacer una predicción solo deben ser sumados los vectores de sorporte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificadores de margen suave\n",
    "* Si existe una separación lineal de las muestras en el RKHS, la SVM de margen dura la encontrará, así se vea no-lineal en el espacio de entrada\n",
    "* Sin embargo, si NO existe separación lineal en el RKHS, el problema de optimización NO tendrá solución\n",
    "* Ademas, si existe **traslape** en las distribuciones condicionales de clase en el espacio de entrada, en el RKHS se forzará la separación lineal, generando sobre-ajustes\n",
    "Se puede hacer que el espacio de caracteristicas sea infinito ($P \\rightarrow infinito$\n",
    "\n",
    "* Para evitar el sobre-ajuste, se permite que existan muestras dentor de la margen con una penalidad, haciendo que la margen sea suave\n",
    "\n",
    "* Se agrega una variable de holgura (slack variable) $e_{n}>=0$ para que la meustra tla que compelte que le falta la muestra debe cumplir que tenga una clasificación correcta que donde un \"prestamo\" de los dato suqe esten en la margen, si estan en la margen deben recibir ese valor **e**\n",
    "\n",
    "* Para meustras en el lado correcto de la margen $\\xi _{n}=0$\n",
    "* Para muestras entre la frontera y la margen de la clase correcta debe ser $0<\\xi _{n}<1$\n",
    "* Par ameustras sobre la frontera por que $y(x_{n})=0$ $\\xi _{n}=1$\n",
    "* Para meustras entr el afrotnera y la margen de la clase incorrecta $\\xi _{n}>|$\n",
    "\n",
    "\n",
    "* Como los penalziadores deben ser las minimas, el nuevo problema primal será que se minimice los \"prestamos\" que se realicen\n",
    "* Si C aumenta, se le da más peso al término de holguras (por la complejidad del modelo)\n",
    "* Si C disminuye, pesa mas la simpleza del modelo\n",
    "* Si $C \\rightarrow infinito$ la maquina elimina por completo las holguras y vuelve la solución de marginalidad dura\n",
    "\n",
    "Por lo tanto, c se comporta como un paramétro de regularazión inverso.\n",
    "\n",
    "al lreemplazar los resultados en el Lagragiano teniendo en cuenta las rstreiccioens y otros proceso snateiores\n",
    "\n",
    "$L(a)=\\frac{1}{2}||w||^{2}-\\sum _{a_{n}t_{n}y(x_{n})}+\\sum _{n=1} ^{N}$ -> igual que la solución de margen dura\n",
    "\n",
    "Teniendo en cuenta las restricciones\n",
    "$a_{n} debe estar entre 0 y C$\n",
    "\n",
    "La solución optima con lo que se tiene hasta ahora es\n",
    "\n",
    "en vez de $a^{*}=(0, a_{2})$ se tiene para margne suave $a^{*}=(0,c)$\n",
    "\n",
    "La mayoria de las veces se utiliza las SVM de margen suave.\n",
    "\n",
    "# El proceso para SVM es\n",
    "* Resolve rla SVM: $\\frac{Nsv}{N}$ Cada vector de soporte que resulta es las caracteristicas en las cuales deben apoyarse para clasificar, si el valor de $\\frac{Nsv}{N}$ da casi igual al total de caracteristicas (tipo 99%) es una maquina demasiado compleja, entonces deberia optarse a aumentar las restricciones para que el valor en $\\frac{Nsv}{N}$ sea reducido\n",
    "* Los parametros de control son el valor **C** y los paramtros del kernel donde incluyen el grado del polinomio, el $\\sigma$\n",
    "\n",
    "Las SVM son mas \"baratas\" para entrenar en el sentido de que son menso costosas en procesamiento, las redes suelen tomar mas caraceristicas que las SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geogebra.org/m/yyxvs6rt\n",
    "\n",
    "https://www.geogebra.org/m/ucyvdvfg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
