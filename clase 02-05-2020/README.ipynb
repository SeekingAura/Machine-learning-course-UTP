{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales\n",
    "\n",
    "## Modelo no lineal\n",
    "\n",
    "* Hasta ahora en el curso, se han considerado modelo de regresión y clasificación que comprenden **combinaciones lineales de funciones base fijas**\n",
    "* Seria interesante poder fijar el número de bases a priori y permitir que se adapten al problema\n",
    "* El modelo más exitoso de este tpo en el contexto del aprendizaje de máquina es el perceptrón multicapa\n",
    "\n",
    "Anteriormente teniamos los datos originales y habian problemas cuando el problema era no-lineal, tambien se tenian los modelos de base radial RBF ene ste se cooloaba una cantidad de centroides probando como la que mejor se creia. En el caso de machine learning seria funciones base te valor fijo (no parametricas es decir que no depende de nada mas) el mejor para ello  es el perceptron multicapa\n",
    "\n",
    "# Historia\n",
    "## S. McCulloch y W. Pitts -1940\n",
    "Desarrollaron el modelo de la \"neurona\" en donde se basa un sistema tipo grafo que a razón de unas entradas que son estimulos tienen una reacción, estos grafos son de peso ajustable y los pesos no implican aprendizaje. En si lo que hacia es que la suma de todos los estimulos se mira si la \"reacción\" supera un umbral acciona a una una proxima neurona\n",
    "\n",
    "## F. Rosenblat - 1950\n",
    "$f(w^{\\top}x)$, este le metió el nombre al tema de neuronas se le colocó el perceptron en donde se daba ya el uso de modelos de representación que son las lineas que representna los modelos.\n",
    "\n",
    "## M. Minsky - Papert - 1970\n",
    "El perceptron tiene un problema que no puede representar ciertos conjuntos de datos que son de tipo no lineal como la compuerta XOR, al demostrarse que una sola linea no puede representar bien los datos \n",
    "\n",
    "## Runellhart - G. Jilton - R. Williams -1980\n",
    "Para solventar el problema que no puede ser solucionado de forma lineal se dió uso del perceptron multi-capa, el cual se basa de utilziar las neurona base para resolver una parte del problema y esa respeust adarselo a otras neuronas que continuan a resolver otra parte dle problema, para que asi las últimas neuronas tengan menor error para dar una respeust,a en caso de que hayan problemas se daba uso del backpropagation \n",
    "\n",
    "## V.Vapnik y C.Cortes 1990\n",
    "De las teorias con los perceptron multicapa se desarrollan las maquinas de soporte vectorial SVM\n",
    "\n",
    "\n",
    "## G.Jinton S. Ruslan - 2000-2010\n",
    "El Deep learning es para colocar mas caracteristicas mediante un sistema que no para de aprende\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes de propagación hacia adelante (FFNN - Feed Forward Neural Network)\n",
    "* Una única neurona se peude escribir como una ecuación lineal seguida de una función de transferencia no lineal\n",
    "\n",
    "$h=f(\\omega^{\\top}\\textbf{x}+b)$\n",
    "\n",
    "$\\textbf{x}\\in R^{D}$\n",
    "\n",
    "* Si unimos varias neuronas, todas alimentadas con la misma entrada **x**, habrían tantas saldias como neuronas pongamos:\n",
    "\n",
    "$\\textbf{h}=\\begin{bmatrix}\n",
    "f(\\omega^{\\top}_{1}\\textbf{x}+b_{1})\\\\ \n",
    "f(\\omega^{\\top}_{2}\\textbf{x}+b_{2})\\\\ \n",
    "\\vdots\\\\ \n",
    "f(\\omega^{\\top}_{M}\\textbf{x}+b_{M})\n",
    "\\end{bmatrix} $\n",
    "\n",
    "$\\textbf{h}=f(\\textbf{W}^{\\top}x+b)$\n",
    "\n",
    "\n",
    "\n",
    "![image](https://i.postimg.cc/NMw7hHVD/Captura-de-pantalla-107.png)\n",
    "\n",
    "Con un modelo multicapa paraPropagación hacia adelante los datos siempre van hacia un solo sentido nunca se regresan (por eso son FFNN, propagación hacia delante)\n",
    "\n",
    "En redes tienen matrices de las mismas caracteristicas permitiendo que se operen de manera recurrente\n",
    "\n",
    "* Cpa de entrada: $h_{0}=x$\n",
    "* Capas internas ocultas: $h_1=f(W^{\\top}_{l}h_{l-1}+b_{l-1})$\n",
    "* Capa de salida: $y=h_{L}$\n",
    "\n",
    "donde $W_{l} \\in R^{M_{l-1}}, h_{l} \\in R^{M_{l}}, b_{l-1} \\in R^{M_{l}}, M_{1}$ correspondiente \n",
    "\n",
    "W y b son los valores que deben ser ajustados\n",
    "\n",
    "$y=\\varphi(x|\\{W_{l}\\}^{L}_{l=1}, \\{b_{l-1}\\}^{L}_{l=1})$\n",
    "\n",
    "* La cantidad de neuronas (nodos) en cada capa y la cantidad de capas constituyen la arquitectura de la capa\n",
    "* Las funcioens de transferencia de salida f se transforman o no, dependiendo del problema a tratar\n",
    "    * Para tareas de regresión se usan funciones lineales $f(\\alpha)=\\alpha$ y tantas neuronas a la salida como variables a predecir sean necesarias:\n",
    "    \n",
    "    $y=W^{\\top}_{L}h_{L-1}+b_{L-1}=W^{\\top}_{L}\\varphi(x)+b_{L-1};y\\in R^{P}$\n",
    "    \n",
    "h es una función no-lieal de x, es una versión generalizada de x. Eso en regresión\n",
    "En tareas de clasficación se usa un softMax, donde se buscan eelmentos entre cero y uno y la suma de los elementos del evector de salida de 1\n",
    "\n",
    "* Para tareas de clasificación se usan funcioens softmax $f(\\alpha)=\\sigma (\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo e MLP (perceptron multicapa)\n",
    "sd\n",
    "![images](https://i.postimg.cc/vTZnMLvr/Captura-de-pantalla-116.png)\n",
    "\n",
    "Para interactuar con un ejemplo\n",
    "\n",
    "https://www.geogebra.org/m/gvkjkrye\n",
    "\n",
    "![images](https://i.postimg.cc/BbRnyx7Q/Captura-de-pantalla-117.png)\n",
    "\n",
    "# Entrenamiento del MLP\n",
    "Esto será un aprendizaje supervizado\n",
    "\n",
    "## Tarea\n",
    "Dado un conjunto de entrenamiento $\\{x_{n}, t_{n}\\}^{N}_{n=1}$ y una arquitectura $y(x)=f(W^{\\top}_{L}fW^{\\top}_{L-1}$ y es entrada x es salida\n",
    "\n",
    "### Funciones Error\n",
    "* Para tareas de regresión, J corresponde al error cuadrático medio:\n",
    "\n",
    "![image3](https://i.postimg.cc/d0JtzQ6d/Captura-de-pantalla-118.png)\n",
    "\n",
    "crossentropia es la cantidad de información cruzada\n",
    "\n",
    "'y' es la probabilidad aposteriori de clase, esta tiene la forma de softmax cuando es de clasificación\n",
    "\n",
    "$\\alpha^{L-1}_{k}$ -> es lo último que le entra al sistem ade clasificación al softmax\n",
    "\n",
    "### Optimización de paramtros\n",
    "Se debe de calcular el gradiente, se supondrá que las muestras en la base de datos no tienen dependencia entre si (estan distribuidas). Esto hace que el error sea la suma de los erorres de cada muestra\n",
    "\n",
    "![image4](https://i.postimg.cc/nzCsPcV2/Captura-de-pantalla-119.png)\n",
    "\n",
    "Para la derivada será aplicada mediante la regla de la cadena \n",
    "\n",
    "En la imagen las derivadas serán calculadas de salida a entrada, empezando desde la de la ultima capa (entre mas oscuro mas lejos es la capa\n",
    "\n",
    "![image5](https://i.postimg.cc/vBJm2rN8/Captura-de-pantalla-122.png)\n",
    "\n",
    "Se deriva cada una de las funciones a razón de cual es la función de **transferencia** el cual puede ser un sigmoide u otras, dpenediendo cual sea será la forma de la muestra (el $f(a)$)\n",
    "\n",
    "![image6](https://i.postimg.cc/zGxGYZcc/Captura-de-pantalla-123.png)\n",
    "\n",
    "Se jutan todas las derivadas realizadas hasta de momento y seria lo de la penultima capa\n",
    "\n",
    "![image7](https://i.postimg.cc/YCCt3DCf/Captura-de-pantalla-126.png)\n",
    "\n",
    "Par ala ante-penultimna capa se hacen procesos parecidos ne lo resaltado de color verde, azul (función de trnasferencia) y amarillo\n",
    "\n",
    "![image8](https://i.postimg.cc/L8nr5cs7/Captura-de-pantalla-127.png)\n",
    "\n",
    "\n",
    "Luego son juntadas todas, La idea es encontrar el patrón\n",
    "\n",
    "![image9](https://i.postimg.cc/15zd7kDt/Captura-de-pantalla-128.png)\n",
    "\n",
    "Con lo identificado se ve que el error se retro-propaga por que va de la salida desde la entrada. En si es el delta de la ultima capa y es multiplicado por el delta de la capa anterior, anterior. Lo resaltado en verde. las ecucaciones\n",
    "\n",
    "![image10](https://i.postimg.cc/pdKcnRM0/Captura-de-pantalla-129.png)\n",
    "\n",
    "\n",
    "### Algoritmo general de retropropagación (BP algorithm):\n",
    "\n",
    "![image10](https://i.postimg.cc/4NMWLmJL/Captura-de-pantalla-130.png)\n",
    "\n",
    "![image11](https://i.postimg.cc/kXDjvxdR/Captura-de-pantalla-131.png)\n",
    "\n",
    "7. Se vuelve a empezar desde el paso 2 hasta que se cumpla algún criterio de convergencia\n",
    "\n",
    "### Consideraciones sobre el algoritmo\n",
    "Las redes neuronales son universales es decir se pueden usar para todo\n",
    "\n",
    "![image12](https://i.postimg.cc/L5W3bbMY/Captura-de-pantalla-133.png)\n",
    "\n",
    "\n",
    "En lo siguiente son regresión para diferentes funciones\n",
    "![image13](https://i.postimg.cc/HWvznV7Y/Captura-de-pantalla-134.png)\n",
    "\n",
    "![image14](https://i.postimg.cc/7YCV2wgg/Captura-de-pantalla-135.png)\n",
    "\n",
    "# Notas sobre redes\n",
    "![image15](https://i.postimg.cc/MTP7nFHP/Captura-de-pantalla-136.png)\n",
    "\n",
    "una a red debe ser entrenada muchas veces\n",
    "\n",
    "\n",
    "Como la red va cambiando a medida que entrena se almacena el peso de los entrenamientos anteriores si el error va empeorando o no mejora entonces se contabiliza la cantidad de iteraciones en los que no ha cambiado (early stopping), si para por ello tiene particularidades\n",
    "\n",
    "![image15](https://i.postimg.cc/VN4MhVqf/Captura-de-pantalla-138.png)\n",
    "\n",
    "\n",
    "* Este algoritmo es parte de una arquitectura está dada (nodos y capas). Para encontrar la mejor arqutiectura, se exploran diferentes y se selecciona la que tenga el mejor desempeño. Si se queire alijerar los requerimientos de la solución se les reduce neuronas que se consideran innecesarias y se re-entrena si es mejor se queda, si empeora se puede devolver.\n",
    "\n",
    "### Métodos de optimización\n",
    "#### Gradiente descendiente estocástico\n",
    "![image16](https://i.postimg.cc/3NGFzZrt/Captura-de-pantalla-140.png)\n",
    "\n",
    "Aqui los datos se parten en grupos y la evaluación se hace solo en los de ese grupo. El gradiente total es la suma de los datos de cada una de las muestras pero la suma solo se peude hacer en el mismo sub-conjunto de datos esto permite que se llegen a soluciones validas mediante menos derivadas.\n",
    "\n",
    "Cada grupo se forma algo llamado mini-batches \n",
    "\n",
    "![image17](https://i.postimg.cc/sDchxLLp/Captura-de-pantalla-142.png)\n",
    "\n",
    "\n",
    "\n",
    "#### Backpropagation con momentum\n",
    "El momentum en fisica se define como el \"impulso\" el empujón que se genera por la fuerza dada anteirormente (una inercia) entonces asi la derivada de cero el seguirá en la dirección que iba.\n",
    "\n",
    "![image18](https://i.postimg.cc/JzzBkqwS/Captura-de-pantalla-143.png)\n",
    "\n",
    "Cada que es movido por ese impulso, ese impulso se va agotando\n",
    "\n",
    "\n",
    "$\\Delta W^{\\tau}_{L}$ -> Es parte de la memoria\n",
    "\n",
    "![image19](https://i.postimg.cc/Ghq4dyBK/Captura-de-pantalla-144.png)\n",
    "\n",
    "#### Backpropagation con decaimiento - Decay\n",
    "![image20](https://i.postimg.cc/13Xft3wQ/Captura-de-pantalla-145.png)\n",
    "\n",
    "#### Métodos Quasi-Newton (BFGS, LBFGS) - Broyden Fletcher Goldfarb Sharp\n",
    "\n",
    "![image21](https://i.postimg.cc/cHyHX7jx/Captura-de-pantalla-149.png)\n",
    "\n",
    "* Queasi-newton\n",
    "![image22](https://i.postimg.cc/kGwgJ05G/Captura-de-pantalla-151.png)\n",
    "\n",
    "\n",
    "#### ADAptive Moment estimation (ADAM)\n",
    "* ADAM es una de las alternativas más recientes (2014) al SGD\n",
    "* Combina las ventajas de AdaGrad y RMSProp\n",
    "    * **AdaGrad** consider auna tasa de aprendizaje a nivel de parámetro (uno por cada peso). Esto a nivel de parametro es como si cada neurona tuviera su propio nivel de aprendizaje (su propio ritmo). Es una tasa de aprendizaje adaptativa\n",
    "    * **Root Mean Square Propagation** usa una tasa de aprendizaje adaptativa basada en la magnitud de los gradientes recientes, es decir, cuantifica ¿Qué tan rapido cambia cada parámetro?\n",
    "* Regla de actualización\n",
    "\n",
    "![image23](https://i.postimg.cc/Prrq356w/Captura-de-pantalla-152.png)\n",
    "\n",
    "$\\eta$ -> tasa de aprendizaje\n",
    "\n",
    "Estas técnicas tambien peuden combinarse para mejorar la calidad de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El zoológico de las redes neuronales\n",
    "\n",
    "https://www.asimovinstitute.org/neural-network-zoo/\n",
    "https://www.youtube.com/watch?v=oJNHXPs0XDk&list=WL&index=83&t=230s\n",
    "\n",
    "En las redes Auto Encode es un caso especial donde se reduce la cantidad de neuronas entre las capas de entrada y salida son de menor cantidad, esto tecnicamente seria una PCA o tecnicas de reducción de dimensión\n",
    "\n",
    "## Redes Deep Convulation network\n",
    "Las redes Deep son idoneas para nalisis en imagenes donde se van mirando datos cada vez mas pequeños en donde al tener el dato mas conciso se extraen las caracteristicas (eso es la convolución)\n",
    "\n",
    "## Redes Deep neural\n",
    "Tambien existe la deconvolución que de un dato pequeño analizar y desplegar muchos datos\n",
    "\n",
    "[image30](https://i.postimg.cc/xjsQdyRX/Captura-de-pantalla-153.png)\n",
    "\n",
    "## Extreme learning\n",
    "Son redes que no tienen un orden de conexión que es a lo que de, en donde lo que hace es tomar los datos que son considerados mas relevantes o utiles\n",
    "\n",
    "![image31](https://i.postimg.cc/L8PFNdMP/Captura-de-pantalla-154.png)\n",
    "\n",
    "## GANS Generative Adversal neural network\n",
    "\n",
    "![image32](https://i.postimg.cc/6QNJPH9m/Captura-de-pantalla-155.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A picture is worth a thousand (coherent) words: building a natural description of images \n",
    "https://ai.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html\n",
    "AlphaGo - The Movie | Full Documentary \n",
    "https://www.youtube.com/watch?v=WXuK6gekU1Y\n",
    "Facebook 10 Year Challenge decoded \n",
    "https://medium.com/@moloishaun5/facebook-10-year-challenge-decoded-3d6963a5fe2b\n",
    "A Style-Based Generator Architecture for GANs - Generating and Tuning Realistic Artificial Faces \n",
    "https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431\n",
    "Image-to-Image Translation with Conditional Adversarial Nets \n",
    "https://phillipi.github.io/pix2pix/\n",
    "\n",
    "Una de las estrategias que uitiliza google con su traductor actualmente es una maquina entrenada para todos los lenguajes traducir al ing le sy en base al ingles traducir a la lengua deseada (por ejemplo traducir de español a protuges seria traducir al ingles y luego al idioma deseado) las redes que se utilizan son mediante imagenes. Mediante el mismo traducor describe lo mejor posible lo que tiene la imagen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
